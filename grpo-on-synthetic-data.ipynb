{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXZLSacyWgdB"
   },
   "source": [
    "## Synthetic Data Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spaMDp3gZEty"
   },
   "outputs": [],
   "source": [
    "# Kaggle-compatible setup (Unsloth + GRPO + Gemma-3)\n",
    "!pip install --upgrade pip -q\n",
    "!pip install unsloth trl peft accelerate bitsandbytes datasets huggingface_hub -q\n",
    "!pip install --no-deps git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2dz_bfeZOyk"
   },
   "source": [
    "## Load up Gemma3-1B and Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MClvfXNaZORH",
    "outputId": "c60818c9-f12b-4194-d90d-5cb2dfd206e2"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "max_seq_length = 1024\n",
    "\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "\n",
    "    # Other popular models!\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/Llama-3.3-70B\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
    "    \"unsloth/Phi-4\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-1b-it\",\n",
    "    max_seq_length = max_seq_length, # Choose any for long context!\n",
    "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwQUy3uHZUJd"
   },
   "source": [
    "Add LORA adaptors to update only a small number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "knIlSwBcZUXM",
    "outputId": "8765106f-9a9c-463a-f026-f731496e258a"
   },
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False,\n",
    "    finetune_language_layers   = True,\n",
    "    finetune_attention_modules = True,\n",
    "    finetune_mlp_modules       = True,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    r = 8,\n",
    "    lora_alpha = 8,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFSPFs4_WQi2",
    "outputId": "f08d22bd-9207-43eb-cd3e-c2a095935fdb"
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_model_state_dict\n",
    "\n",
    "# Check if LoRA weights are being tracked\n",
    "peft_state = get_peft_model_state_dict(model)\n",
    "print(f\"LoRA parameters being trained: {len(peft_state)}\")\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUYMHxO0ZcaQ"
   },
   "source": [
    "## Data Preparation of Synthetic Data\n",
    "\n",
    "This data is taken from Hugging Face, created from [Gretel Navigator with meta-llama](https://huggingface.co/datasets/gretelai/gsm8k-synthetic-diverse-8b). It contains ~1500 Training and 300 Test Grade School Math Problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g1nVXZqxbL1W",
    "outputId": "4035cfc7-c826-4fe6-a336-d7668f7bf7b0"
   },
   "outputs": [],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUBArjPMZuqw"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "def load_synthetic_gsm8k(split=\"train\"):\n",
    "    base_path = \"hf://datasets/gretelai/gsm8k-synthetic-diverse-8b/data/\"\n",
    "    file_map = {\n",
    "        \"train\": \"train-00000-of-00001.parquet\",\n",
    "        \"test\": \"test-00000-of-00001.parquet\"\n",
    "    }\n",
    "    if split not in file_map:\n",
    "        raise ValueError(\"Split must be 'train' or 'test'\")\n",
    "\n",
    "    df = pd.read_parquet(base_path + file_map[split])\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nrrg5diIbe_B",
    "outputId": "37e179a6-fb4c-4fb6-dc21-fbf897a79e8b"
   },
   "outputs": [],
   "source": [
    "train_set = load_synthetic_gsm8k(\"train\")\n",
    "test_set = load_synthetic_gsm8k(\"test\")\n",
    "\n",
    "print(f\"Train set size: {len(train_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")\n",
    "\n",
    "# View a sample\n",
    "print(\"\\nTrain example:\")\n",
    "print(train_set[0])\n",
    "\n",
    "print(\"\\nTest example:\")\n",
    "print(test_set[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gS0MHI-GdEST"
   },
   "source": [
    "### Check the structure with question & answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "CwqiHAbgdBSo",
    "outputId": "25487091-5f9c-4252-fede-17279de58c00"
   },
   "outputs": [],
   "source": [
    "train_set[0][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "ipIw4xbtc5JT",
    "outputId": "63398a99-b464-4409-c825-8a7ce82d4218"
   },
   "outputs": [],
   "source": [
    "train_set[0][\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGQ-F5pFgio3"
   },
   "source": [
    "### Notice the answer has a ####, so we extract that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "IGs0Eg5Egp6_",
    "outputId": "0368001f-6c51-4212-d5b1-4b520ea55912"
   },
   "outputs": [],
   "source": [
    "def extract_hash_answer(text):\n",
    "    if \"####\" not in text: return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "extract_hash_answer(train_set[0][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTD9g0irgwGC"
   },
   "source": [
    "We now create a system prompt which can be customized. We add 4 extra symbols for working out or thinking / reasoning sections and a final answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "vVPIN7JUcsFe",
    "outputId": "9b727fd3-9f8a-465f-911e-af5fb146ff8c"
   },
   "outputs": [],
   "source": [
    "reasoning_start = \"<start_working_out>\"\n",
    "reasoning_end   = \"<end_working_out>\"\n",
    "solution_start = \"<SOLUTION>\"\n",
    "solution_end = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
    "system_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j02E4Gk_g3hL"
   },
   "source": [
    "Let's map our synthetic dataset and observe the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414,
     "referenced_widgets": [
      "e48cb9308d7b4a11a1a4cf0f6af207e4",
      "b40de2c10c0e42f8a2e4e25c731256e5",
      "7e6bbd31f79a4bd0987ad47624e40bcd",
      "020f904fb67840088607bf14ee3ee5d1",
      "132c087a32d64dde95622b8debf0b455",
      "1d8caa6442444b9d9fd6d11a196a5f48",
      "f76ceb38d0ae43b99d3cedd6616f9da4",
      "e41d7c4c25194effa44c446c240a5ede",
      "2b2ef85ca11b46459d6fbcc0bf83c78d",
      "ab99d363e1c44c4ca3f554f5c8ba2760",
      "492c74e598e041aba54b9e96cf21d8df"
     ]
    },
    "id": "15OXZ7N7g2-H",
    "outputId": "fe305e76-1bb3-4bde-c87d-66a3f5c2b4ee"
   },
   "outputs": [],
   "source": [
    "# map training dataset\n",
    "train_dataset = train_set.map(lambda x: {\n",
    "    \"prompt\" : [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": x[\"question\"]},\n",
    "    ],\n",
    "    \"answer\": extract_hash_answer(x[\"answer\"]),\n",
    "})\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361,
     "referenced_widgets": [
      "ea22ba2c2fb749408818cd9fc5249ece",
      "df19cd3971664fdea0dd85a486480a49",
      "a2173d7a61bc4ff295656c081a4ee2ab",
      "8801cf3c02a94ed28c8e23155be38d18",
      "bd1e8e5efe6349a08b45c4d5de5676fd",
      "ac597e7f19dd40ef8a774f2cb5ce30e2",
      "ed31fff8d0154ef38a41d8b69f8cf086",
      "c6548df991084e0cb52a08767fb7b090",
      "be7d72c9f35548698974723816da9d72",
      "b86f656ec95143299fe15947ec1fe286",
      "f6279e77cdbb4db3b0a000af721bdc5a"
     ]
    },
    "id": "7UMHRUSFh0cK",
    "outputId": "7d33e2c3-dd47-4d36-c09d-c5e84f3b99ef"
   },
   "outputs": [],
   "source": [
    "test_dataset = test_set.map(lambda x: {\n",
    "    \"prompt\" : [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": x[\"question\"]},\n",
    "    ],\n",
    "    \"answer\": extract_hash_answer(x[\"answer\"]),\n",
    "})\n",
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNyTYGNVh8c7"
   },
   "source": [
    "We create a regex format to match the reasoning sections and answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysRNi6kgh9Bb"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\\\n",
    "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\\\n",
    "    rf\"{solution_start}(.+?){solution_end}\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DCl_d6siAu4",
    "outputId": "d21970e0-083a-472d-a9d0-86a20518ee7a"
   },
   "outputs": [],
   "source": [
    "# verify that it works\n",
    "match_format.search(\n",
    "    \"<start_working_out>Let me think!<end_working_out>\"\\\n",
    "    \"<SOLUTION>2</SOLUTION>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MHO_CUtiG8d"
   },
   "source": [
    "Create a reward function to match the format exactly - we reward it with 3 points if it succeeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPfy0_yuiGVM"
   },
   "outputs": [],
   "source": [
    "def match_format_exactly(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Match if format is seen exactly!\n",
    "        if match_format.search(response) is not None: score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xzKRWkQPiKM8"
   },
   "outputs": [],
   "source": [
    "# if it fails, give it partial rewards\n",
    "def match_format_approximately(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Count how many keywords are seen - we penalize if too many!\n",
    "        # If we see 1, then plus some points!\n",
    "        score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
    "        score += 0.5 if response.count(reasoning_end)   == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_start)  == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_end)    == 1 else -0.5\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVsg114FiTP7"
   },
   "source": [
    "Finally, we want to extract the generated answer, and reward or penalize it! We also reward it based on how close the answer is to the true one via ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nGOG0i1iSsO"
   },
   "outputs": [],
   "source": [
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_format.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        # Correct answer gets 3 points!\n",
    "        if guess == true_answer:\n",
    "            score += 3.0\n",
    "        # Match if spaces are seen\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score += 1.5\n",
    "        else:\n",
    "            # We also reward it if the answer is close via ratios!\n",
    "            # Ie if the answer is within some range, reward it!\n",
    "            try:\n",
    "                ratio = float(guess) / float(true_answer)\n",
    "                if   ratio >= 0.9 and ratio <= 1.1: score += 0.5\n",
    "                elif ratio >= 0.8 and ratio <= 1.2: score += 0.25\n",
    "                else: score -= 1.0 # Penalize wrong answers\n",
    "            except:\n",
    "                score -= 0.5 # Penalize\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DomXhfi7iWbB",
    "outputId": "c56f84df-d0f2-4758-eaf4-ec1df329bf1d"
   },
   "outputs": [],
   "source": [
    "# The answer may not come as a single number, let's account for that:\n",
    "match_numbers = re.compile(\n",
    "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "match_numbers.findall(\"<SOLUTION>  0.34  </SOLUTION>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOEP2HlxiiPA"
   },
   "outputs": [],
   "source": [
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_numbers.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    print('*'*20, f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        # Convert to numbers\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            guess       = float(guess.strip())\n",
    "            scores.append(1.5 if guess == true_answer else 0.0)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w959gvacik6U"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Training the model\n",
    "\n",
    "Now set up GRPO Trainer and all configurations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ndxc44DgTOsj"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import upload_folder\n",
    "from transformers import TrainerCallback\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login first\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUh2QRQhTQ6a"
   },
   "outputs": [],
   "source": [
    "class HFPushCallback(TrainerCallback):\n",
    "    def __init__(self, model, repo_id):\n",
    "        self.model = model\n",
    "        self.repo_id = repo_id\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        step = state.global_step\n",
    "        adapter_path = f\"checkpoint_lora_step_{step}\"\n",
    "        print(f\"üíæ Saving LoRA at {adapter_path} and uploading to HF...\")\n",
    "\n",
    "        # Save LoRA adapter\n",
    "        self.model.save_lora(adapter_path)\n",
    "\n",
    "        # Push to Hugging Face\n",
    "        upload_folder(\n",
    "            repo_id=self.repo_id,\n",
    "            folder_path=adapter_path,\n",
    "            repo_type=\"model\",\n",
    "            path_in_repo=f\"lora_step_{step}\",  # Subfolder inside repo\n",
    "            commit_message=f\"Checkpoint at step {step}\"\n",
    "        )\n",
    "\n",
    "        print(f\"‚úÖ Uploaded LoRA checkpoint to {self.repo_id}/lora_step_{step}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRpopGm5in9N",
    "outputId": "bc166cfa-d960-403c-e6d0-474ab765664f"
   },
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    learning_rate = 5e-6,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_torch_fused\",\n",
    "    logging_steps = 10,\n",
    "\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 4,\n",
    "\n",
    "    num_generations = 2,\n",
    "    max_prompt_length = 192,\n",
    "    max_completion_length = 192,\n",
    "\n",
    "    max_steps = 200,\n",
    "    save_steps = 50,\n",
    "    save_total_limit = 2,\n",
    "\n",
    "    max_grad_norm = 0.3,\n",
    "    report_to = \"none\",\n",
    "    output_dir = \"outputs_synth_grpo_fullrun\",\n",
    "\n",
    "    bf16 = False,  # P100 doesn't support this\n",
    "    fp16 = True,   # Use fp16 for better memory efficiency\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\",   \"content\": \"What is the sqrt of 101?\"},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    tokenize = False,\n",
    ")\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"gemma-3\")  # Local saving\n",
    "tokenizer.save_pretrained(\"gemma-3\")\n",
    "model.push_to_hub(\"etoileboots/gemma-3\", token = \"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\") # Online saving\n",
    "tokenizer.push_to_hub(\"etoileboots/gemma-3\", token = \"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\") # Online saving\n",
    "model.push_to_hub_merged(\n",
    "        \"etoileboots/gemma-3-full-finetune\", tokenizer,\n",
    "        token = \"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\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"etoileboots/gemma-3-full-finetune\"  # assuming it's the merged model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "import torch\n",
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(\"What is 12 multiplied by 7?\", return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# Simple input\n",
    "prompt = \"What is 12 multiplied by 7?\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "    \n",
    "# Print response\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- Load synthetic GSM8K test split ---\n",
    "def load_synthetic_gsm8k(split=\"test\"):\n",
    "    base_path = \"hf://datasets/gretelai/gsm8k-synthetic-diverse-8b/data/\"\n",
    "    file_map = {\n",
    "        \"train\": \"train-00000-of-00001.parquet\",\n",
    "        \"test\": \"test-00000-of-00001.parquet\"\n",
    "    }\n",
    "    df = pd.read_parquet(base_path + file_map[split])\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    return dataset\n",
    "\n",
    "# --- Format prompt with system + user messages ---\n",
    "def format_prompt(q): \n",
    "    return [\n",
    "        {'role': 'system', 'content': \"Respond with step-by-step reasoning and a final answer.\"},\n",
    "        {'role': 'user', 'content': q}\n",
    "    ]\n",
    "\n",
    "# --- Extract final number from output ---\n",
    "def extract_number(text):\n",
    "    match = re.findall(r\"[-+]?[0-9]*\\.?[0-9]+\", text)\n",
    "    return match[-1] if match else \"\"\n",
    "\n",
    "# --- Normalize answers for comparison ---\n",
    "def normalize(x):\n",
    "    return re.sub(r\"[^a-zA-Z0-9]\", \"\", x.lower().strip())\n",
    "\n",
    "# --- Load and prepare data ---\n",
    "test_data = load_synthetic_gsm8k(\"test\").select(range(50))  # üîÅ Adjust range for more data\n",
    "prompts = [format_prompt(ex[\"question\"]) for ex in test_data]\n",
    "gold_answers = [ex[\"answer\"].split(\"####\")[-1].strip() for ex in test_data]\n",
    "\n",
    "# --- Set tokenizer left padding (decoder-only fix) ---\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# --- Run inference ---\n",
    "preds, outputs = [], []\n",
    "for prompt in tqdm(prompts):\n",
    "    # üß† Step 1: Format the prompt properly\n",
    "    text = tokenizer.apply_chat_template(prompt, tokenize=False)\n",
    "\n",
    "    # üß† Step 2: Tokenize manually\n",
    "    tokenized = tokenizer(text, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    tokenized = {k: v.to(model.dtype) if v.dtype == torch.float else v for k, v in tokenized.items()}\n",
    "\n",
    "    # üß† Step 3: Generate\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**tokenized, max_new_tokens=128)\n",
    "\n",
    "    # üß† Step 4: Decode\n",
    "    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    outputs.append(decoded)\n",
    "    preds.append(extract_number(decoded))\n",
    "\n",
    "\n",
    "# --- Score EM Accuracy ---\n",
    "scores = [int(normalize(p) == normalize(g)) for p, g in zip(preds, gold_answers)]\n",
    "accuracy = sum(scores) / len(scores)\n",
    "print(f\"\\nüî• Exact Match Accuracy on Synthetic Test Set: {accuracy:.2%}\")\n",
    "\n",
    "# --- Save Results ---\n",
    "df = pd.DataFrame({\n",
    "    \"question\": [ex[\"question\"] for ex in test_data],\n",
    "    \"gold_answer\": gold_answers,\n",
    "    \"predicted_answer\": preds,\n",
    "    \"raw_output\": outputs,\n",
    "    \"exact_match\": scores,\n",
    "})\n",
    "df.to_csv(\"synthetic_test_eval.csv\", index=False)\n",
    "print(\"‚úÖ Saved evaluation to 'synthetic_test_eval.csv'\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
